{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2: Text Preprocessing Pipeline (Enhanced)\n",
                "\n",
                "## Introduction\n",
                "We will explore Tokenization, Stopword Removal, and Stemming. In this enhanced version, we will use interactive widgets to see how these steps affect text in real-time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install nltk ipywidgets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "import micropip\n",
                "await micropip.install('nltk')\n",
                "\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Interactive Preprocessing Pipeline\n",
                "Turn switches on/off to see how the text changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ipywidgets as widgets\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "ps = PorterStemmer()\n",
                "\n",
                "def preprocess(text, remove_stops, stem):\n",
                "    words = word_tokenize(text)\n",
                "    \n",
                "    if remove_stops:\n",
                "        words = [w for w in words if w.lower() not in stop_words]\n",
                "        \n",
                "    if stem:\n",
                "        words = [ps.stem(w) for w in words]\n",
                "        \n",
                "    print(\"Processed Tokens:\", words)\n",
                "\n",
                "text_input = widgets.Textarea(value=\"The quick brown fox jumps over the lazy dog.\", description=\"Text:\")\n",
                "check_stops = widgets.Checkbox(value=False, description=\"Remove Stopwords\")\n",
                "check_stem = widgets.Checkbox(value=False, description=\"Apply Stemming\")\n",
                "\n",
                "widgets.interactive(preprocess, text=text_input, remove_stops=check_stops, stem=check_stem)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (Pyodide)",
            "language": "python",
            "name": "python"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}